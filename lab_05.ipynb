{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b75d73a0-fe9b-4f1c-b84b-e3527f372044",
   "metadata": {},
   "source": [
    "# APIs and SQL\n",
    "\n",
    "*This notebook inclues adapted content from [Melanie Walsh's chapter on Data Collection](https://melaniewalsh.github.io/Intro-Cultural-Analytics/04-Data-Collection/00-Data-Collection.html).*\n",
    "\n",
    "In this lab, we'll introduce a useful way to extract data from online, as well as a canonical tool used to explore large datasets when you don't have access to a Python environment. We'll go over the following topics:\n",
    "\n",
    "- Accessing an API\n",
    "- API Wrappers\n",
    "- SQL and SQLite\n",
    "- pandasql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e13dbf4-a2b2-4bdf-827c-c747e5b9c53c",
   "metadata": {},
   "source": [
    "# APIs\n",
    "\n",
    "It seems only natural that we should be able to extract any data from the internet by programmatically logging information after \"going\" to each website you're interested in. (In fact, it is [perfectly legal](https://melaniewalsh.github.io/Intro-Cultural-Analytics/04-Data-Collection/01-User-Ethics-Legal-Concerns.html).) One way to do this is using [web scraping](https://melaniewalsh.github.io/Intro-Cultural-Analytics/04-Data-Collection/02-Web-Scraping-Part1.html), where you write an algorithm which parses website content, logs data, and loops through several HTML web pages. But, this method is becoming less effective over the years, as websites are becoming far more complex (harder to scrape), and most companies are transitioning to a platform where their data is more easily accessible (and controlled) in an Application Programming Interface (API).\n",
    "\n",
    "## What is an API?\n",
    "\n",
    "**An API allows you to programmatically extract and interact with company data which drives their websites.** In this way, social networks, museums, foundations, research labs, applications, and projects can make their data publicly available, allowing for developers to use the data to build applications and tools (e.g., for your phone, computer, or refrigerator) that can be used by the general populous. For example, the reason you can access Google Maps on your phone is because developers used the Google Maps API to build that functionality.\n",
    "\n",
    "Of course, there are plenty of companies or foundations which will likely never use APIs to store/access their data. In these cases though, you can usually find an API that is *related* to that website, or someone may have built (or, they are building) a third-party API for that purpose. Web scraping should typically be a last resort, so we do not teach it in this class.\n",
    "\n",
    "<span style=\"color:red\">**Caveat:** People typically design their APIs such that they decide exactly which kinds of data they want to share. So, they often choose not to share their most lucrative and desirable data. In those cases, you are usually asked to pay some fee.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e33714-bac3-45e6-a5b9-b9b48734d763",
   "metadata": {},
   "source": [
    "## Using Environment Variables\n",
    "\n",
    "We will discuss environment variables more in a future lesson, but to use APIs properly, we need to have at least a basic understanding of what environment variables are.\n",
    "\n",
    "When working on any data science project (e.g., like the web app you'll build later in this course), you will likely track your progress using Git/GitHub. But, keys and secret strings (like the ones we will use to access an API) should never be pushed to GitHub. Instead, it's a best practice to use environment variables when dealing with this kind of data. In short, environment variables are values stored in a special file on your local computer, or on the cloud where your project may be hosted. In this way, those variables are only accessible to agents with access to that file (e.g., your Python interpreter, or the one on the cloud).\n",
    "\n",
    "In this class, we will use [dotenv](https://github.com/theskumar/python-dotenv#getting-started) to manage environment variables for API. You'll need to `pip` install it, as directed in the instructions, then create a file with the name *.env* (notice the period) in your project directory to hold any keys or secrets. Since we're going to use this package in this notebook, we'll import the library here. *Note: If you're using Git/GitHub, make sure \".env\" is added to your [.gitignore file](https://www.atlassian.com/git/tutorials/saving-changes/gitignore)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56f39204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a64ff7a-970f-4c77-a417-dfa82ddc7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510ca951-7360-4eaa-b072-1e774ac42cb2",
   "metadata": {},
   "source": [
    "## Accessing an API\n",
    "\n",
    "The steps to access *any* API are about the same, no matter the API. So, in this lesson, we're going to use the [Genius](https://genius.com/) API to access data about songs.\n",
    "\n",
    "### Step 1: Client Access Token\n",
    "\n",
    "Typically, to use an API, you need a special API key usually called a \"Client Access Token\", which is kind of like a password. Many APIs require authentication keys to gain access to them. To get your necessary Genius API keys, follow these steps:\n",
    "\n",
    "1. Navigate to the [api-clients page](https://genius.com/api-clients) (which will prompt you to [sign up for an account](https://genius.com/signup_or_login) if you haven't already). Then, click the button that says **\"New API Client\"**.\n",
    "2. Remember, APIs are expecting *developers* to use their APIs to build applications (e.g., for your phone, computers, etc.). But, since we're only doing data analysis for a college course in informatics, we only need to fill in the fields for \"App Name\" (e.g., *\"Song Lyrics Project\"*), and \"App Website URL\" (e.g., *\"https://github.com/leontoddjohnson/i501\"*). Then, click **Save**.\n",
    "3. When you click \"Save,\" you'll be given a series of API Keys: a \"Client ID\" and a \"Client Secret.\" **Copy/Paste these values into your *.env* file** without quotations, as instructed in the dotenv documentation. For example, my *.env* file looks something like this:\n",
    "    CLIENT_ID=asdfghjkl;123456789\n",
    "    CLIENT_SECRET=qwertyuiop098765432\n",
    "6. To generate your \"Client Access Token,\" which is the API key that we'll be using in this notebook, you need to click \"Generate Access Token\". Place that in your *.env* file as you did the other variables, maybe under the variable name ACCESS_TOKEN.\n",
    "\n",
    "We can access our `ACCESS_TOKEN` by using *dotenv* to load our environment variables into the current environment, then with the built-in Python *os* library to access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ed65d04-7b84-4207-9c93-43d6442f4fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4062fb72-9067-4515-b654-9f736f16c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# do not print this variable anywhere if the notebook is going on GitHub\n",
    "ACCESS_TOKEN = os.environ['ACCESS_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31042682-c1d1-4ff0-8861-7a22d68000d5",
   "metadata": {},
   "source": [
    "### Step 2: Making an API Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7881f4cb-ce72-4a06-a52e-4494eee327ff",
   "metadata": {},
   "source": [
    "Making an API request is very similar to accessing a URL in your browser. But, instead of getting a rendered HTML web page in return, you get some data in return.\n",
    "\n",
    "There are a few different ways that we can [query the Genius API](https://docs.genius.com/#songs-h2), but here we'll use [the basic search](https://docs.genius.com/#search-h2), which allows you to get a bunch of Genius data about any artist or songs that you search for:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42c185a-b0e5-4bdb-bb63-d05f031c8521",
   "metadata": {},
   "source": [
    "`http://api.genius.com/search?q={search_term}&access_token={client_access_token}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56954ef1-ee53-4647-8668-5e5e2806540e",
   "metadata": {},
   "source": [
    "First we're going to assign the string \"Missy Elliott\" to the variable `search_term`. Then we're going to make an f-string URL that contains the variables we'd like to include in our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb4d58a-fbf6-448f-9169-b85417752e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"Missy Elliott\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56727e16-04af-4054-ad31-337b3c0f8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "genius_search_url = f\"http://api.genius.com/search?q={search_term}&access_token={ACCESS_TOKEN}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa7c65d-c9f9-4641-96d1-d249a57bbbc4",
   "metadata": {},
   "source": [
    "You can see the data we'll be requesting from this API by printing the `genius_search_url`, pasting it into your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45b85a2-50ae-4d32-8ad2-5e08d0586f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(genius_search_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5f0536-bb4b-4d0d-8758-6b6b3e7c076c",
   "metadata": {},
   "source": [
    "The data you might see when you navigate to your URL is in [JSON](https://www.w3schools.com/whatis/whatis_json.asp) format. JSON is an acronym for JavaScript Object Notation, and it is a data format commonly used by APIs. JSON data can be nested, and contains key-value pairs, much like a Python dictionary.\n",
    "\n",
    "We can access this JSON directly in Python using the [`requests` library](https://requests.readthedocs.io/en/latest/) to send HTTP requests to a remote client. If you like, you can read more about what a \"request\" is [here](https://developer.mozilla.org/en-US/docs/Web/HTTP/Overview), but it suffices to say that it represents an online communication between your computer and the server storing the data you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b71e3f-6150-47a7-b9c6-17574d411523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51608402-4516-4716-8e76-d8decdd8721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, we make a \"GET\" request to the Genius server\n",
    "response = requests.get(genius_search_url)\n",
    "json_data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a973df3d-3888-45f4-9675-fc82ae8b5f73",
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198e743-333f-48ee-b989-fcfef454ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data['response'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08888967-d792-433c-a382-eec134e22e96",
   "metadata": {},
   "source": [
    "Genius places all of its search results into the \"hits\" element. By default, it looks like it returns at most 10 search results for any request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f3fbd9-65bc-4724-bee8-b0a029c46652",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(json_data['response']['hits'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f313ba-5830-4c90-9dbe-bd73471a1c90",
   "metadata": {},
   "source": [
    "According to the documentation, we can use [referents](https://docs.genius.com/#referents-h2) to increase that number to a maximum of 20 results per request using `per_page`. With this slight adjustment added, let's consolidate our request into a single function to use again later. We'll also add this to our *api_util.py* file for the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb436c7-b52b-4baf-aeb6-310051383a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genius(search_term, per_page=15):\n",
    "    '''\n",
    "    Collect data from the Genius API by searching for `search_term`.\n",
    "    \n",
    "    **Assumes ACCESS_TOKEN is loaded in environment.**\n",
    "    '''\n",
    "    genius_search_url = f\"http://api.genius.com/search?q={search_term}&\" + \\\n",
    "                        f\"access_token={ACCESS_TOKEN}&per_page={per_page}\"\n",
    "    \n",
    "    response = requests.get(genius_search_url)\n",
    "    json_data = response.json()\n",
    "    \n",
    "    return json_data['response']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081343a8-acd1-4f19-bfa7-ea345c6ea5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = genius(\"The Beatles\")\n",
    "len(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa2aeec-446a-446e-962b-58dd6d373dfa",
   "metadata": {},
   "source": [
    "## Loading JSON Data Into a DataFrame\n",
    "\n",
    "For us to efficiently work with the JSON data, we need to load them into a DataFrame. Using panda's [read_json function](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html), we can do just that in a pretty efficient way. The only detail is we need the JSON to be in one of the acceptable orientations (see the `orient` argument in the documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb002e9-e8db-4543-9c1d-46ef98d06cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a0c04f-e124-40bb-bbd3-0b33f7282bae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "json_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583d8f30-2170-414e-95f6-c5587ab5268c",
   "metadata": {},
   "source": [
    "When we look at any of the hits, we see the data we're interested in is contained in the `\"result\"` element. We can consolidate all of the \"result\" elements for each \"hit\" using a list comprehension. We then use the `json` library to convert this list of JSONs into a single JSON.\n",
    "\n",
    "**Looking ahead:** Notice that the `\"stats\"` and the `\"primary_artist\"` elements contain *dictionaries* of interesting data that we'll need to unpack once we have our data into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6382b5bc-e046-444c-861a-7b00560cacf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = [hit['result'] for hit in json_data]\n",
    "hits_json = json.dumps(hits)\n",
    "\n",
    "# load JSON into DataFrame\n",
    "df = pd.read_json(hits_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c6725-61cf-4eaa-a306-fd4941a8b7c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6115181b-c6c5-4446-bc8d-2125e1f5c6cf",
   "metadata": {},
   "source": [
    "Recall that `\"stats\"` and `\"primary_artist\"` contain dictionaries which we want to unpack. After a bit of StackOverflow searching (say), we find that we can [use](https://stackoverflow.com/a/38231651) `pd.apply(pd.Series)` and `pd.concat` to explode these into columns. We'll need to make a slight adjustment to the column names to avoid repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecb0308-b927-4d07-be56-dffbbf3e45ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = df['stats'].apply(pd.Series)\n",
    "df_stats.rename(columns={c:'stat_' + c for c in df_stats.columns},\n",
    "                inplace=True)\n",
    "\n",
    "df_primary = df['primary_artist'].apply(pd.Series)\n",
    "df_primary.rename(columns={c:'primary_artist_' + c for c in df_primary.columns},\n",
    "                  inplace=True)\n",
    "\n",
    "df = pd.concat((df, df_stats, df_primary), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f177904a-032f-4328-9a84-9e52cbb40ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['stat_unreviewed_annotations', 'stat_hot', 'stat_pageviews']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a90b81-7a58-49fe-aba1-bec1dc311fda",
   "metadata": {},
   "source": [
    "### Collecting Multiple API Calls\n",
    "\n",
    "We are going to want to perform analysis on more than one artist, so let's use what we've written above to collect data from multiple API calls by looping through multiple search terms. When we loop through each search term, we use the [tqdm package](https://pypi.org/project/tqdm/) to help us visualize our progress (you may need to use `pip` to install it). This kind of thing is helpful when we're running multiple API calls, and we don't know how long it will take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e815200-8988-454a-9ac1-e8d65898fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76eb86c-e345-46ec-b9c7-e525f828ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['The Beatles', 'Missy Elliot', 'Andy Shauf', 'Slowdive', 'Men I Trust']\n",
    "n_results_per_term = 10\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# loop through search_terms in question\n",
    "for search_term in tqdm(search_terms):\n",
    "    json_data = genius(search_term, per_page=n_results_per_term)\n",
    "    hits = [hit['result'] for hit in json_data]\n",
    "    hits_json = json.dumps(hits)\n",
    "\n",
    "    # load JSON into DataFrame\n",
    "    df = pd.read_json(hits_json)\n",
    "\n",
    "    # expand dictionary elements\n",
    "    df_stats = df['stats'].apply(pd.Series)\n",
    "    df_stats.rename(columns={c:'stat_' + c for c in df_stats.columns},\n",
    "                    inplace=True)\n",
    "    \n",
    "    df_primary = df['primary_artist'].apply(pd.Series)\n",
    "    df_primary.rename(columns={c:'primary_artist_' + c for c in df_primary.columns},\n",
    "                      inplace=True)\n",
    "    \n",
    "    df = pd.concat((df, df_stats, df_primary), axis=1)\n",
    "    \n",
    "    # add to list of DataFrames\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64284218-534a-4bd4-8d0e-35ae103a06a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genius = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462ef83-792e-4c29-9949-128cc5b9baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genius.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f16d46-451f-4ea0-afc1-fc18720f8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genius.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9152b3-d97c-4982-ae67-519d693cfa5c",
   "metadata": {},
   "source": [
    "Of course, we'll want to copy this function into (or build it *within* our IDE in) our .py file.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f43e934-cc15-460c-89d0-793d148111fc",
   "metadata": {},
   "source": [
    "## Using an API Wrapper\n",
    "\n",
    "More often than not, someone has built an \"API Wrapper\" for the API you are working with. An API wrapper makes an API easier to use, and it often extends the API itself. It will typically consist of classes and functions similar to the ones we've built above, but spanning a wide range of functionality and access to the API. For example, John Miller's [LyricsGenius](https://github.com/johnwmillr/LyricsGenius) gives us an almost universal access to the Genius website, and it even uses web scraping to collect song lyrics themselves.\n",
    "\n",
    "<span style=\"color: darkblue\">**If ever you're working with an API, do some Googling to make sure there isn't a wrapper you can use to make things easier on you!**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b486f7-bbb3-414a-8cb6-6da6f3b097cb",
   "metadata": {},
   "source": [
    "First, we'll [install LyricsGenius](https://github.com/johnwmillr/LyricsGenius#installation) *(in 2023, there is no conda option, so we would need to use `pip`)*, then import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b39b6c-cadc-4908-90d7-8e9790f99f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b9390e-f396-4554-9f50-523c21fa1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an \"API Class\" is typical for API wrappers\n",
    "LyricsGenius = lyricsgenius.Genius(ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef709a-a86f-418f-af78-4c89ba791c1a",
   "metadata": {},
   "source": [
    "To get the top songs and song lyrics from a specific artist you can use the method `.search_artist()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317e2f99-091c-4bec-b4dc-cedeb4bd615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist = LyricsGenius.search_artist(\"Missy Elliott\", max_songs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf094bd-40ee-4360-8434-74ea919eb9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(artist.songs[0].lyrics[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56c9919-ef70-408d-ba8b-c2cffb8435da",
   "metadata": {},
   "source": [
    "You'll notice this function took *much* longer than our function above. If you take a quick glance at the documentation for the function (use Shift+Tab in the parentheses next to the function), you'll see that the `get_full_info=True` argument slows down the search (likely because it includes scraping lyrics). If we were using the lyrics in our investigation, we might be okay with this delay, but since we're only interested in numerical data for the time being (and because setting `get_full_info=False` is still a bit slow), we'll continue using the process we built above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b171d446-77d4-4afe-8cf0-d14a3fccb03e",
   "metadata": {},
   "source": [
    "# SQL\n",
    "\n",
    "*<span style=\"color:darkred;\">Section Prerequisites: [SQLBolt](https://sqlbolt.com/) lessons 1-6 (and, 7-12 if possible).</span>*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380a8eb-dc20-4b0e-b17e-cdb2ae315b6d",
   "metadata": {},
   "source": [
    "[Structured Query Language (SQL)](https://www.sqltutorial.org/what-is-sql/) is a programming language designed to allow users to **query** databases containing multiple tables (rows and columns) of data, each related to one another using column-to-column relationships. It is easily the most popular language for accessing large tabular databases, so naturally, many companies and users have used the language to create their own variants of the language called [dialects](https://arctype.com/blog/sql-dialects/). In this class, we will be using [SQLite](https://www.sqlite.org/index.html) and its corresponding dialect.\n",
    "\n",
    "In the job setting, you will typically make SQL queries using a combination of the following:\n",
    "\n",
    "- Some data browser, with data stored on the cloud or on in-house servers\n",
    "- Python, likely using a package called SQLAlchemy to connect to a database\n",
    "\n",
    "In this class, we will use [DB Browser](https://sqlitebrowser.org/about/) to simulate the kind of browser you'd use on the job, and SQLAlchemy to gain practice with the tool. We'll also see how you can use the [pandasql package](https://pypi.org/project/pandasql/) to assimilate SQL queries into the pandas framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261ceb90-8257-4dc7-9fee-a38f74d05f77",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "\n",
    "### SQLite\n",
    "\n",
    "Before we can continue, you need to make sure that SQLite is installed on your machine.\n",
    "\n",
    "- For MacOS users, SQLite should already be installed on your computer. You can test this by running `sqlite3 --version` in your terminal.\n",
    "- For everyone else, you'll need to [follow these steps to download and install SQLite](https://www.sqlitetutorial.net/download-install-sqlite/).\n",
    "\n",
    "Lastly, for this lab, we're going to use the [SQLite Sample Database](https://www.sqlitetutorial.net/download-install-sqlite/). Scroll down to the \"Download SQLite sample database\" section of the page for the link, or download it directly [here (as of Sep 2023)](https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip). Unzip the file, and move the .db file to a convenient location (e.g., the same place where this lab is saved). *Note: If you are using GitHub, \".db\" should typically be added to your [.gitignore file](https://www.atlassian.com/git/tutorials/saving-changes/gitignore).*\n",
    "\n",
    "### SQLAlchemy and pandasql\n",
    "\n",
    "Lastly, we'll be using [SQLAlchemy](https://www.sqlalchemy.org/) to connect our Python environment to our database, and [pandasql](https://pypi.org/project/pandasql/) to use SQL \"in\" pandas. You can install them both using anaconda:\n",
    "\n",
    "    pip install SQLAlchemy \n",
    "    pip install -U pandasql\n",
    "\n",
    "### DB Browser \n",
    "\n",
    "Next, you'll need to [install DB Browser](https://sqlitebrowser.org/dl/) byÂ following the installation instructions provided for your particular operating system. *Note: For Mac \"M1/M2\" chips, you'll use the \"Apple Silicon\" option.*\n",
    "\n",
    "Once installed, double click on the DB Browser for SQLite icon to open up the application on your computer. On MacOS, you may get an error message saying that the application can't be opened because Apple cannot check it for malicious software. To resolve, right click on the DB Browser for SQLite icon and click Open instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd39241-a901-4b1d-bd77-9d6cdf5e3817",
   "metadata": {},
   "source": [
    "**Exploring in DB Browser**\n",
    "\n",
    "Let's explore the `chinook` database (i.e., the SQLite Sample Database) using DB Browser to \"test\" queries, and SQLAlchemy (below) to run queries here in the notebook.\n",
    "\n",
    "1. First, in DB Browser, click the \"Open Database\" button, then find the *chinook.db* file on your computer. (This is how you would open any SQL \".db\" file.)\n",
    "2. Close the side panels on the right until all you see is the Main window and the handy \"DB Schema\" viewer on the right. The [SQL Schema](https://www.sqlite.org/schematab.html) provides information on the tables and columns within a database. *Note: you can also view a SQL database schema directly using [PRAGMA commands](https://www.sqlite.org/pragma.html).*\n",
    "3. Select the \"Execute SQL\" tab to start writing SQL Queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5d7f10-6be7-4153-889a-d1c63f11ea6c",
   "metadata": {},
   "source": [
    "## SQLAlchemy\n",
    "\n",
    "Again, you can use DB Browser to explore your data, but you can also *bring that data into your notebook* using a combination of SQLAlchemy and pandas.\n",
    "\n",
    "### Connect\n",
    "\n",
    "To connect to a database using SQLAlchemy, we need to define the database location. Using `create_engine`, we create a connection between the SQL database represented in the *.db* file, and our Python instance.\n",
    "\n",
    "*Note: In our case, we have a database immediately accessible on our computer. But in practice, you'll more likely need to [access a remote database](https://docs.sqlalchemy.org/en/20/core/engines.html#custom-dbapi-connect-arguments-on-connect-routines) requiring credentials.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb9e3b-306b-4ff5-9d28-c16e26fb06f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # if you haven't already, above\n",
    "\n",
    "from sqlalchemy import inspect, create_engine\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b5ad1-b097-4cae-9799-e2f5b03e7131",
   "metadata": {},
   "source": [
    "For this lab, the database is stored in a *data* folder inside the same directory as this notebook. This notebook has a \"working directory\" or file path associated with it, which can be used by Python to \"navigate\" to the same location. Using the same `os` library from above, we can use `os.getcwd()` to get the **current working directory**, and use it to navigate to the *chinook* database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee85b2-1dbb-4dad-a2ef-5e4b42449bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "db_path = cwd + \"/data/chinook.db\"  # complete path to the database file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce8f9d8-7569-430b-ae32-d40bd2531517",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T21:33:13.013582Z",
     "start_time": "2020-10-27T21:33:13.005757Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Uq-HjwqjaW22",
    "outputId": "b4e8e41e-6108-4f32-bd76-6f58dfbb55d5"
   },
   "outputs": [],
   "source": [
    "# the \"engine\" is a connection between Python and the database\n",
    "engine = create_engine(f\"sqlite:////{db_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc3ca6-73a7-4aec-9b02-3dce3a7c5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is one way to access an aspect of the schema\n",
    "insp = inspect(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a9593-7a93-4d5c-901f-829da0c7f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "insp.get_table_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4273104-3203-429b-a8d6-1b258a576b13",
   "metadata": {
    "id": "E5yVCEj1gq-T"
   },
   "source": [
    "### Load into pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26510ec4-04a4-4b8b-a7f2-ac85b52730f5",
   "metadata": {
    "id": "qaAkofG0DDaj"
   },
   "source": [
    "Once you have a connection between Python and the database, we can use `pd.read_sql()` to load the result of SQL queries into pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0386eae-7d67-40e2-883b-1536d993a87f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T21:36:07.517081Z",
     "start_time": "2020-10-27T21:36:07.476093Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "rSUR5KgXBfJF",
    "outputId": "61af4e00-fc07-46ce-f286-2ffd74eece23"
   },
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "    DISTINCT(city)\n",
    "FROM employees;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb801170-bfc9-4773-8364-b08bca7d6842",
   "metadata": {},
   "source": [
    "## SQL Statements\n",
    "\n",
    "<span style=\"color: darkred;\">*This section contains code examples from the [SQLite Tutorial Website](https://www.sqlitetutorial.net/). Refer to this website for more in-depth explanations.*</span>\n",
    "\n",
    "We interact with SQL using a \"query\", or the code/interface between the user and the database. It contains keywords, column names, tables names, and even function operations. In this notebook, we will introduce a couple of examples of some common query statements, and then follow up with a few more methods SQL provides.\n",
    "\n",
    "*Note: **SQL code is case-insensitive**, but I find it to be a good practice to capitalize keywords (e.g., \"SELECT\"), and lower the case of column names (e.g., \"employees\", above) when using SQL. I also tend to use different lines and indenting wherever possible to keep the code clean.*\n",
    "\n",
    "### [SELECT](https://www.sqlitetutorial.net/sqlite-select/)\n",
    "\n",
    "The foundation of virtually all SQL queries is the `SELECT` statement. Typically, this is followed (at some point) by a `FROM`, denoting (naturally) where we are selecting our data from. `DISTINCT` removes duplicate rows of a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a13ce-10be-43de-b0b1-4eed3d272a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "    DISTINCT city\n",
    "FROM employees;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02be373-2df0-40e8-8ba2-30581510ad46",
   "metadata": {},
   "source": [
    "### [LIMIT](https://www.sqlitetutorial.net/sqlite-limit/)\n",
    "\n",
    "`LIMIT` only returns the first set of rows for a result, very much like `head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a54567-12c2-4432-877d-98fd01c857a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "\tname,\n",
    "\tcomposer,\n",
    "\tmilliseconds,\n",
    "\tunitprice\n",
    "FROM tracks\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0e997-e8e9-41d2-85d0-72191d97d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "    name,\n",
    "    composer,\n",
    "    unitprice\n",
    "FROM tracks\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d6ab1f-b17b-45ec-8c0b-4b3f9b4095f9",
   "metadata": {},
   "source": [
    "### [ORDER BY](https://www.sqlitetutorial.net/sqlite-order-by/)\n",
    "\n",
    "We can also order our data based on some column (akin to \"sort\", in pandas). Sort will default to ascending order, but it's a good practice to include `ASC` or `DESC` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be1e5a-369e-4e91-9c6c-9e02bfc0bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "\tname,\n",
    "\tmilliseconds, \n",
    "\talbumid\n",
    "FROM\n",
    "\ttracks\n",
    "ORDER BY\n",
    "\talbumid ASC,\n",
    "    milliseconds DESC;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae2d897-f816-42c8-87a1-b20f0f6b7492",
   "metadata": {},
   "source": [
    "*Note: the column you use to sort your data *does not* need to be included in the SELECT statement.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad148d55-9a5e-4f9c-9277-36e97c801c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "\tname,\n",
    "\tcomposer,\n",
    "\talbumid\n",
    "FROM\n",
    "\ttracks\n",
    "ORDER BY\n",
    "    milliseconds DESC\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf9378-d0fe-42a0-8ed5-16e184302966",
   "metadata": {},
   "source": [
    "Using `LIMIT` in tandem with `ORDER BY` helps us extract the `n`th item (highest or lowest), ordered by some column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc8c11b-4c2a-45d2-b2dc-67fc84054cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second longest track\n",
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "\ttrackid,\n",
    "\tname,\n",
    "\tmilliseconds\n",
    "FROM\n",
    "\ttracks\n",
    "ORDER BY\n",
    "\tmilliseconds DESC\n",
    "LIMIT 1 OFFSET 2;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657dff7f-b428-44b1-9e6a-ee01874a55ad",
   "metadata": {},
   "source": [
    "### [WHERE](https://www.sqlitetutorial.net/sqlite-where/)\n",
    "\n",
    "We can *filter* our data using the `WHERE` clause. In the same way that pandas provides logical operations, there are also several available in SQLite (see the section link above for more on these, and [this article on \"glob\" operators](https://www.sqlitetutorial.net/sqlite-glob/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b73b5-e46a-4d82-aada-4afd2c8c9761",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "\tname,\n",
    "\talbumid,\n",
    "\tMilliseconds,\n",
    "\tmediatypeid\n",
    "FROM\n",
    "\ttracks\n",
    "WHERE\n",
    "\tmediatypeid IN (2, 3)\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b5b01-c6c2-4cab-9637-fe63f4306675",
   "metadata": {},
   "source": [
    "The `%` wildcard can be handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553fea70-7218-4a4f-b3eb-f4d534cfdaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "\tname,\n",
    "\talbumid,\n",
    "\tcomposer\n",
    "FROM\n",
    "\ttracks\n",
    "WHERE\n",
    "\tcomposer LIKE '%Smith%'\n",
    "ORDER BY\n",
    "\talbumid;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f9c184-325c-474f-bff2-a25c71e62087",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "\tname,\n",
    "\tmilliseconds,\n",
    "\tbytes,\n",
    "\talbumid\n",
    "FROM\n",
    "\ttracks\n",
    "WHERE\n",
    "\talbumid = 1\n",
    "\tAND milliseconds > 250000;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23a585b-8472-4c00-bc01-e6d4c57fcfb7",
   "metadata": {},
   "source": [
    "### [IS (NOT) NULL](https://www.sqlitetutorial.net/sqlite-is-null/)\n",
    "\n",
    "Of course, we may want to include or exclude missing values in the data. In SQL, \"missing\" values are encoded as `NULL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40751771-7993-4224-9cc1-0c7c45a275fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "    Name, \n",
    "    Composer\n",
    "FROM\n",
    "    tracks\n",
    "WHERE\n",
    "    Composer IS NOT NULL\n",
    "ORDER BY \n",
    "    Name;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdb5391-c7df-444a-a939-c64654426be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "\tInvoiceId,\n",
    "\tBillingCity,\n",
    "\tBillingState,\n",
    "\tBillingPostalCode,\n",
    "\tTotal\n",
    "FROM invoices\n",
    "WHERE\n",
    "\tBillingState IS NULL\n",
    "\tAND BillingPostalCode IS NULL\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeb3717-64a6-42c4-8d21-a1f2f82f784e",
   "metadata": {},
   "source": [
    "### [JOIN](https://www.sqlitetutorial.net/sqlite-join/)\n",
    "\n",
    "The SQL JOIN allows us to merge data from multiple tables. It's essentially the same thing as the `pandas.merge`, but the code is a bit more accessible than pandas when it comes to merging many tables together.\n",
    "\n",
    "One common practice is for databases to have an \"entity\" table, which contains the ID along with many other attributes of the entity. Then, in a table, one only needs to reference the ID of the entity rather than store redundant data that exists already in another table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ed1210-3d79-4abc-8446-76a5fc95e4b9",
   "metadata": {},
   "source": [
    "#### [INNER JOIN](https://www.sqlitetutorial.net/sqlite-inner-join/)\n",
    "\n",
    "The `INNER JOIN` only matches rows where the column value in question exists in **both** tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e5de7-de8f-45b7-8e13-2d64e949f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT \n",
    "    ar.Name artist_name,\n",
    "    al.Title AS album_title\n",
    "FROM \n",
    "    albums al\n",
    "INNER JOIN artists ar\n",
    "    ON al.ArtistId = ar.ArtistId\n",
    "ORDER BY ar.Name\n",
    "LIMIT 15;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3b099d-2a63-49d5-84cf-44ff8b9aafb1",
   "metadata": {},
   "source": [
    "A few things to note here:\n",
    "\n",
    "- the `AS` keyword (or a space) followed by some string or keyword allows us to change the way data is presented in the result of a query, such as for renaming columns or tables.\n",
    "- Whenever we join multiple tables, it's a good practice to \"name\" those tables (using the `AS`/space syntax), then reference columns with the `table.column` notation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8eba20-d230-42dd-8bde-52160a208e62",
   "metadata": {},
   "source": [
    "#### [LEFT JOIN](https://www.sqlitetutorial.net/sqlite-left-join/)\n",
    "\n",
    "Similarly, the `LEFT JOIN` collects all rows where the value in question exists in the \"left\" table, regardless of whether it exists in the \"right\" table. (Left and right here are defined by the order in which tables occur on either side of the word `JOIN`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd72ea8c-5d9e-4f2a-8249-9f1909d5a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "    ar.Name artist_name,\n",
    "    al.Title album_title\n",
    "FROM\n",
    "    artists ar\n",
    "LEFT JOIN albums al ON\n",
    "    ar.ArtistId = al.ArtistId\n",
    "WHERE al.Title IS NULL   \n",
    "ORDER BY Name\n",
    "LIMIT 5;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c55b3e0-7425-485e-aafd-04a18dae4d36",
   "metadata": {},
   "source": [
    "#### [CROSS JOIN](https://www.sqlitetutorial.net/sqlite-cross-join/)\n",
    "\n",
    "The `CROSS JOIN` collects all combinations of values between two columns in a table. This kind of function is handy when you want to calculate something for multiple groups based on all the values that exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26e142-fd64-4abf-9d8a-55692f118123",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT * \n",
    "FROM media_types\n",
    "CROSS JOIN genres\n",
    "LIMIT 50;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5749a59e-2290-4155-a508-f7e11b161378",
   "metadata": {},
   "source": [
    "#### [FULL OUTER JOIN](https://www.sqlitetutorial.net/sqlite-full-outer-join/)\n",
    "\n",
    "The `FULL OUTER JOIN` collects the *union* of all rows which have matching columns values between tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f54c0d-c59c-4e5d-a271-81014d506bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "    ar.Name artist_name,\n",
    "    al.Title album_title\n",
    "FROM\n",
    "    artists ar\n",
    "FULL OUTER JOIN albums al ON\n",
    "    ar.ArtistId = al.ArtistId\n",
    "ORDER BY Name\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "print(\"Number of rows: \", df_result.shape[0])\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bfaf00-152b-4762-bdca-bde983cae10f",
   "metadata": {},
   "source": [
    "*Note: there are only 275 rows in the `artists` table, and 347 in the `albums` table. We'll see how you can calculate these values shortly!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb6fe13-7cd7-42ad-8450-c2ade05c68b5",
   "metadata": {},
   "source": [
    "#### [SELF JOIN](https://www.sqlitetutorial.net/sqlite-self-join/)\n",
    "\n",
    "The `SELF JOIN` is just a join between a table and itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b94968-2001-4fb1-9508-f9b64564635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT m.firstname || ' ' || m.lastname AS 'manager',\n",
    "       e.firstname || ' ' || e.lastname AS 'direct_report' \n",
    "FROM employees e\n",
    "INNER JOIN employees m\n",
    "    ON m.employeeid = e.reportsto\n",
    "ORDER BY manager;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780a7b5d-3f51-4bef-be4a-ef9c42871f74",
   "metadata": {},
   "source": [
    "### [GROUP BY](https://www.sqlitetutorial.net/sqlite-group-by/)\n",
    "\n",
    "The `GROUP BY` function exists across many data manipulation frameworks (e..g, R, pandas, etc.), and it is meant to break up the data into groups. Typically, once data is broken into groups, continuous values are aggregated to a single value within each group. SQL provides many [aggregation functions](https://www.sqlitetutorial.net/sqlite-aggregate-functions/) which can be used with `GROUP BY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f33dc9-bb95-4178-90e9-6cbffa4398d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "\talbumid,\n",
    "\tCOUNT(trackid)\n",
    "FROM\n",
    "\ttracks\n",
    "GROUP BY\n",
    "\talbumid\n",
    "ORDER BY COUNT(trackid) DESC;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32081b58-364d-403a-a912-533bb1ebd717",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "\tt.albumid AS album_ID,\n",
    "\ta.title AS album_name,\n",
    "\tCOUNT(t.trackid) AS num_track_ids\n",
    "FROM\n",
    "\ttracks t\n",
    "INNER JOIN albums a\n",
    "    ON a.albumid = t.albumid\n",
    "GROUP BY\n",
    "\tt.albumid\n",
    "ORDER BY\n",
    "\tnum_track_ids DESC\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba23c5d-7a2a-4959-aacb-cc0b00e73e36",
   "metadata": {},
   "source": [
    "### [HAVING](https://www.sqlitetutorial.net/sqlite-having/)\n",
    "\n",
    "The `HAVING` operator is the `WHERE` operator which we can apply *after* the `GROUP BY`. That is, the \"Group By Section\" of a query has keywords in this order: `WHERE` $\\to$ `GROUP BY` $\\to$ `HAVING`.\n",
    "\n",
    "A good way to remember this is that the word \"having\" makes more sense if you think about it as applied to *collections* (or groups) of things rather than the things themselves (e.g., \"I *have* a handful of marbles\", not \"I *where* a handful of marbles\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd249ceb-ab59-4b83-b3a5-c438f85937d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "   albumid,\n",
    "   COUNT(trackid)\n",
    "FROM\n",
    "   tracks\n",
    "GROUP BY\n",
    "   albumid\n",
    "HAVING \n",
    "   COUNT(albumid) BETWEEN 18 AND 20\n",
    "ORDER BY albumid;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fba7f7-972a-4963-a0ed-8a2cf62937b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "\tar.name AS artist_name,\n",
    "\ta.title AS album_name,\n",
    "\tCOUNT(trackid) AS num_tracks\n",
    "FROM\n",
    "\ttracks t\n",
    "INNER JOIN albums a\n",
    "\tON t.albumid = a.albumid\n",
    "LEFT JOIN artists ar\n",
    "\tON a.ArtistId = ar.ArtistId\n",
    "WHERE\n",
    "\tartist_name LIKE \"%Jam%\"\n",
    "GROUP BY\n",
    "\ta.ArtistId,\n",
    "\tt.albumid\n",
    "HAVING\n",
    "\tnum_tracks > 10\n",
    "ORDER BY\n",
    "\tartist_name ASC,\n",
    "\tnum_tracks DESC\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc253b57-52d1-4c10-b176-c52005b20c94",
   "metadata": {},
   "source": [
    "### [CASE](https://www.sqlitetutorial.net/sqlite-case/)\n",
    "\n",
    "The SQL `CASE` statement is the analog for if-then-else operations in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5486ea-64d9-476f-a9a1-a8c5a77f4b7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT customerid,\n",
    "\tfirstname,\n",
    "\tlastname,\n",
    "    country,\n",
    "\tCASE country \n",
    "\t\tWHEN 'USA' \n",
    "\t\t\tTHEN 'Domestic' \n",
    "\t\t\tELSE 'Foreign' \n",
    "\tEND CustomerGroup\n",
    "FROM \n",
    "    customers\n",
    "ORDER BY \n",
    "    LastName,\n",
    "    FirstName\n",
    "LIMIT 20;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238c27a1-f0a7-4a0e-99cf-23ca40c3dd35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "\ttrackid,\n",
    "\tname,\n",
    "\tCASE\n",
    "\t\tWHEN milliseconds < 60000\n",
    "\t\t\tTHEN 'short'\n",
    "\t\tWHEN milliseconds > 60000 \n",
    "\t\tAND milliseconds < 300000\n",
    "\t\t\tTHEN 'medium'\n",
    "\t\tELSE\n",
    "\t\t\t'long'\n",
    "\t\tEND category\n",
    "FROM\n",
    "\ttracks\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daa4c69-f399-4e95-8637-a7e13a0b3051",
   "metadata": {},
   "source": [
    "## Subqueries and Views\n",
    "\n",
    "Sometimes, it's helpful to *use* the result of one query *within* another query. This is typically called a [**Subquery**](https://www.sqlitetutorial.net/sqlite-subquery/).\n",
    "\n",
    "- The [(NOT) EXISTS](https://www.sqlitetutorial.net/sqlite-exists/) operator checks whether a subquery returns a result at all.\n",
    "- If a subquery is overly complex, or if you plan to use it in the future, you can save it  as a [**view**](https://www.sqlitetutorial.net/sqlite-create-view/) (or, you can [delete](https://www.sqlitetutorial.net/sqlite-drop-view/) one you no longer need).\n",
    "\n",
    "As an example, below we have a subquery which returns the track information for only a particular album."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ead7d-2abd-4849-873e-02057b72dad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT trackid,\n",
    "       name,\n",
    "       albumid\n",
    "FROM tracks\n",
    "WHERE albumid = (\n",
    "   SELECT albumid\n",
    "   FROM albums\n",
    "   WHERE title = 'Let There Be Rock'\n",
    ");\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea68e45c-773f-46ca-9e0f-8e375d6d1f9d",
   "metadata": {},
   "source": [
    "### Set Operations\n",
    "\n",
    "In SQL, there are also set operations [UNION](https://www.sqlitetutorial.net/sqlite-union/), [EXCEPT](https://www.sqlitetutorial.net/sqlite-except/) (i.e., set difference), and [INTERSECT](https://www.sqlitetutorial.net/sqlite-intersect/). For each of these, you'd use subqueries to build the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72866055-d80a-4874-a0b0-c06965fbae54",
   "metadata": {},
   "source": [
    "### EXERCISE\n",
    "\n",
    "Take a look at these different set operations. Can you build a query which returns a **single column** of the unique album names *and* artist names which contain the word \"black\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bbe787-fb6d-4c7f-af59-e3ebb273daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085689d1-2fd3-4ac3-9db2-7a9081f2f737",
   "metadata": {},
   "source": [
    "## SQL Functions\n",
    "\n",
    "It's rare that we are satisfied with the data as it exists within the data table. Typically, we want to transform the data, and present it in a certain way. This is where SQL Functions come in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1d1245-9df0-4255-878c-72458bde2c5d",
   "metadata": {},
   "source": [
    "### Mathematical Operations\n",
    "\n",
    "SQLite has several different [data types](https://www.sqlitetutorial.net/sqlite-data-types/), and sometimes, we'd like to leverage one type over another. Suppose we'd rather show the number of minutes rather than milliseconds. We can use the `CAST` operator to convert our value to a `FLOAT`, or we can divide *by* a float (e.g., `60000.0`) to coerce our data into the more complex `FLOAT` data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba7dff2-fb43-4cc0-9fe9-9181cc7561da",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "\tname,\n",
    "\talbumid,\n",
    "\tCAST(Milliseconds AS FLOAT) / 60000 minutes,\n",
    "\tmediatypeid\n",
    "FROM\n",
    "\ttracks\n",
    "WHERE\n",
    "\tmediatypeid IN (2, 3)\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e062334d-7978-4656-aaa0-372f126579e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "\tname,\n",
    "\talbumid,\n",
    "\tROUND(Milliseconds / 60000.0, 3) AS minutes,\n",
    "\tmediatypeid\n",
    "FROM\n",
    "\ttracks\n",
    "WHERE\n",
    "\tmediatypeid IN (2, 3)\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7928ae-da77-40f6-90a6-1090868ef715",
   "metadata": {},
   "source": [
    "### [Date Functions](https://www.sqlitetutorial.net/sqlite-date-functions/)\n",
    "\n",
    "Dates come with their own \"numerical\" representation which can be operated on. In SQL, we can calculate different date-based values using datetime functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5e9162-df40-49ac-bc59-15a87c38cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "\tLastName,\n",
    "\tFirstName,\n",
    "\ttitle,\n",
    "\tBirthDate,\n",
    "\tstrftime('%m', BirthDate) BirthMonth,\n",
    "\tHireDate\n",
    "FROM employees\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7390f750-3bae-4d9f-b0e6-5888c886c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "\tLastName,\n",
    "\tFirstName,\n",
    "\ttitle,\n",
    "\tHireDate,\n",
    "    DATE(HireDate,\n",
    "\t\t'start of month', \n",
    "\t\t'+1 month', \n",
    "\t\t'-1 day') last_day_of_hire_month\n",
    "FROM employees;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c4c0a-e3be-4543-ac28-e42bd7e4693a",
   "metadata": {},
   "source": [
    "### [String Functions](https://www.sqlitetutorial.net/sqlite-string-functions/)\n",
    "\n",
    "Strings are very versitile, and SQL has plenty of operations for handling them. For example, we can use `LENGTH` to determine the lengths of the names for some of these playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced31806-339c-4ba3-8b12-919374f837ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "\tName name,\n",
    "\tLENGTH(name) name_length\n",
    "FROM playlists\n",
    "ORDER BY name_length DESC\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b2beda-ea85-4875-b70a-95f36ef92b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "\tFirstName,\n",
    "\tLastName,\n",
    "\tREPLACE(\n",
    "\t\tREPLACE(Title, \"Manager\", \"Boss\"),\n",
    "\t\t\"IT\", \"Computer\") slang_title\n",
    "FROM employees;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0ea5ed-f263-4d65-88d3-804c97428562",
   "metadata": {},
   "source": [
    "### [Window Functions](https://www.sqlitetutorial.net/sqlite-window-functions/)\n",
    "\n",
    "Window functions perform calculations on rows of data **based on their row-index**. For instance, we might want to know how a row of data compares to others with values lower than it, or maybe a just the index of the row itself, or even a cumulative sum. The syntax for the query looks a bit like this:\n",
    "\n",
    "```sqlite\n",
    "SELECT\n",
    "    ...,\n",
    "    [SOME_EXPRESSION](columns_of_stuff)   --<-- Here lies the \"window function\"\n",
    "        OVER (                            --<-- Apply this function *OVER* some window\n",
    "            PARTION BY ... ) AS ...       --<-- Define the \"window\" and the final column name\n",
    "FROM ...\n",
    "```\n",
    "\n",
    "We perform operations on the records that are inside the window. The `PARTITION` tells you what is included in the window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8483e201-080d-4c33-b15a-f757393e1769",
   "metadata": {},
   "source": [
    "For instance, we can use it to perform a similar task as `.transform`. This query tells us how far a `Total` Invoice amount is from the average total for its city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45831f48-27e9-4754-a5e1-78838b2558d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT \n",
    "    CustomerId,\n",
    "    InvoiceDate,\n",
    "    BillingCity,\n",
    "    Total - AVG(Total) OVER (\n",
    "        PARTITION BY BillingCity\n",
    "      ) AS diff_from_city_avg\n",
    "FROM invoices\n",
    "ORDER BY CustomerId, InvoiceDate;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e98d6a-a6b2-4910-a75c-8b769e763fb8",
   "metadata": {},
   "source": [
    "Or, we can use it to calculate a **cumulative** sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63282921-d200-49d3-b003-b3c423a893b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT \n",
    "\tCustomerId,\n",
    "    InvoiceDate,\n",
    "    BillingCity,\n",
    "\tTOTAL,\n",
    "    SUM(Total) OVER (\n",
    "        PARTITION BY CustomerId \n",
    "        ORDER BY InvoiceDate\n",
    "    ) AS customer_running_total\n",
    "FROM invoices\n",
    "ORDER BY CustomerId, InvoiceDate\n",
    "LIMIT 20;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc284387-881e-4b7a-b042-db0717098492",
   "metadata": {},
   "source": [
    "And (among other things), we could assign a rank to each track of each album based on the length of that track in comparison to the others in the album."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a520f64-6b4d-4438-a215-7d3472fb84b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT\n",
    "\tName,\n",
    "\tMilliseconds,\n",
    "\tAlbumId,\n",
    "\tRANK () OVER ( \n",
    "\t\tPARTITION BY AlbumId\n",
    "\t\tORDER BY Milliseconds DESC\n",
    "\t) LengthRank \n",
    "FROM tracks\n",
    "LIMIT 50;\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql(query, engine)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a269b312-82c6-4e48-8146-2464060acfd2",
   "metadata": {},
   "source": [
    "# EXERCISES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1221ef6a-afa8-4151-a955-6164173104b0",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "In fact, it looks like we can use the `page` referent to capture up to 20 results for *multiple pages* of results (think of scrolling through search results), and append each page to a collection of final results. Is this possible? If so, adjust the above function to include the `page` referent in Genius to return more than 20 results for a search term. If not, explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9457c1-8bf9-4468-b0f4-3b27683fb3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebda72e-bd86-4eb1-b217-545a90aefc78",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Write a SQL query which provides the minimum, maximum, and average track count of albums for each genre. So, each row should be a genre, and the columns would reflect the minimum track count, maximum track count, and average track count. *Feel free to use DB Brower as your \"scratchpad\" to test out your code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61abdd4-3336-4fed-a820-12085234b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd2b3fd-1d3a-4a9d-b4cd-89c270bc7082",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Using window functions and the `chinook` database, write a query which tells us the time between each invoice for each customer in the `invoices` table. E.g., you might have a column that says \"time_since_last_invoice\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e18aa-e504-4109-ba20-6d79d716676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d18852-58cd-4a87-8d33-076874dae5a3",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Take a look at the documentation for [pandasql](https://pypi.org/project/pandasql/). Load in any data frame of your choosing, and select a column that best represents a unique identifier for each row. E.g., if my data frame contains a list of customers, I might use the customer name or customer ID. Then, use *pandasql* to run a SQL query which performs a self join on your data based on that unique identifier column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daafda06-f25d-48b5-89d7-207f28a52e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
